{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8838ae-c4cc-44f7-a4f0-237e8e7651bd",
   "metadata": {},
   "source": [
    "# Data Exploration for Insights\n",
    "\n",
    "This session will guide you through various techniques for examining data to gain deeper insights, build intuition, and initiate hypothesis testing. By the end of this workshop, you'll have a robust set of data exploration skills, laying a solid foundation for any subsequent data analysis or machine learning projects.\n",
    "\n",
    "The data used in this notebook was generated by 'DATA_datasetCompilation.ipynb'. I encourage you to review that notebook to understand the data exploration approach taken for data compilation and cleaning.\n",
    "\n",
    "Please ensure you have the following data files stored in the 'INPUTS/' folder:\n",
    "1. CQ_Data_20240604.txt\n",
    "2. WtshdAttributeTable_20240603.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a928c-bc34-4172-9bcb-10a21da1789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries at the top of your code so you can easily see and organize all the packges you are using. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Setting some global parameters based on my preferences\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04083a55-d355-4715-9781-cd8e33775232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your filepaths\n",
    "inputDataFilepath = 'XXX/INPUTS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5a29e-67f7-4044-ba35-59093cdf4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the concentration, flow, and attribute data.\n",
    "CQ = pd.read_csv(inputDataFilepath+'CQ_Data_20240604.txt', sep=\",\", dtype={'SITE': str, 'HUC02': str})\n",
    "WtshdAtt = pd.read_csv(inputDataFilepath+'WtshdAttributeTable_20240603.txt', sep=\",\", dtype={'SITE': str, 'HUC02': str})\n",
    "\n",
    "# Merging CQ data with their watershe attributes\n",
    "CQ_attrib = CQ.merge(WtshdAtt, on=['SITE','YEAR'], how='left')\n",
    "CQ_attrib = CQ_attrib.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c25b85-dedb-4284-b31d-5da5d88050d3",
   "metadata": {},
   "source": [
    "## Exploring Concentration-Discharge Relationships\n",
    "Today we will be looking at nitrogen concentration (C) and discharge (Q). The relationship between solute and discharge have been explored extensively because they are useful to understand the underlying drivers of catchment export dynamics and are valuable for interpreting the processes driving export patterns of solutes.\n",
    "\n",
    "Different typologies of CQ relationships provide integrated signals of processes hapenning within the catchment. \n",
    "\n",
    "Some foundational literature on CQ relationships in human-impacted landscapes:\n",
    "- [Musolff, A., Schmidt, C., Selle, B., & Fleckenstein, J. H. (2015). Catchment controls on solute export. Advances in Water Resources, 86, 133–146.](https://www.sciencedirect.com/science/article/abs/pii/S030917081500233X)\n",
    "- [Basu, N. B., Destouni, G., Jawitz, J. W., Thompson, S. E., Loukinova, N. V., Darracq, A., Zanardo, S., Yaeger, M., Sivapalan, M., Rinaldo, A., & Rao, P. S. C. (2010). Nutrient loads exported from managed catchments reveal emergent biogeochemical stationarity. Geophysical Research Letters, 37(23).](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2010GL045168)\n",
    "- [Speir, S. L., Rose, L. A., Blaszczak, J. R., Kincaid, D. W., Fazekas, H. M., Webster, A. J., Wolford, M. A., Shogren, A. J., & Wymore, A. S. (2024). Catchment concentration–discharge relationships across temporal scales: A review. WIREs. Water, 11(2). https://doi.org/10.1002/wat2.1702](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/wat2.1702)\n",
    "\n",
    "### Understanding your watersheds\n",
    "Before jumping into finding that patterns and trends we see, let's take a look at our data. We want to explore the spatial coverage and type of watersheds that are in our dataset. This step will help guide us on the type of questions or hypothesis we can ask with the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b85a96-46ae-4076-980e-4ff1ca85b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the watershed shapefile. \n",
    "wtshd = gpd.read_file(inputDataFilepath+'boundaries-shapefiles-by-aggeco/bas_all.shp')\n",
    "wtshd = wtshd.rename(columns={'GAGE_ID':'SITE'})\n",
    "US_boundary = gpd.read_file(inputDataFilepath+'cb_2017_us_state_5m/cb_2017_us_state_5m.shp')\n",
    "wtshd_f = wtshd[wtshd['SITE'].isin(CQ_attrib['SITE'])]\n",
    "US_boundary = US_boundary.to_crs(wtshd_f.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb3d5-50c4-411e-a7b2-9e5650efc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare all GAGE watersheds to the dataset that has water quality samples. \n",
    "\n",
    "# Task: Plot the watershed shapefile and the filtered watershed shapefile. \n",
    "# Syntax: shapefile.plot(ax=[basemap], color='colorName')\n",
    "\n",
    "# I've plotted the base map, you plot the other 2.\n",
    "base = US_boundary.plot(color='white', edgecolor='black')\n",
    "\n",
    "wtshd.plot(ax=base, color='sienna', edgecolor='none')\n",
    "wtshd_f.plot(ax=base, color='darkcyan', edgecolor='none')\n",
    "base.set_xlim(-3*10**6, 3*10**6)\n",
    "base.set_ylim(0, 3.5*10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6162199-1724-41b3-a996-a41f4cabdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of watersheds in the dataset:',len(wtshd_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cb58c-16b9-46b7-bd80-15ecb169d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many datapoints does each watershed have? What is the distribution of years?\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 5))\n",
    "noWtshd = CQ_attrib.groupby('SITE')['Conc'].count()\n",
    "noWtshd = wtshd_f.merge(noWtshd, on='SITE')\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "US_boundary.plot(color='white', edgecolor='black', ax=ax1)\n",
    "noWtshd.plot(ax=ax1, \n",
    "             column='Conc', \n",
    "             cmap='OrRd',\n",
    "             legend = True,\n",
    "             vmin=0,\n",
    "             vmax=100,\n",
    "             edgecolor='none',\n",
    "             legend_kwds={\"label\": \"Number of observations\", \"orientation\": \"horizontal\"})\n",
    "\n",
    "# Contiguous US\n",
    "base.set_xlim(-3*10**6, 3*10**6)\n",
    "base.set_ylim(0, 3.5*10**6)\n",
    "\n",
    "# What years we do we have in our dataset?\n",
    "noYears = CQ_attrib.groupby('YEAR')['Conc'].count().reset_index() # Note: reset index turning the index of the Series into a column in the DataFrame\n",
    "noYears.columns = ['YEAR', 'Conc_Count']\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.barplot(data=noYears, x='YEAR', y='Conc_Count', ax=ax2)\n",
    "ax2.set_ylabel('Number of observations')\n",
    "ax2.set_xticks(['1980', '1990', '2000', '2010', '2020'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78235a75-f0e8-4629-96c0-0e0713c05910",
   "metadata": {},
   "source": [
    "That's some decent coverage! We have watersheds across the entire contiguous US, including some of the more data sparse watersheds in the west. \n",
    "\n",
    "Let's look at the attributes and understanding the distribution of our data. First we will start with the univariate approach, which just means looking at one variable at a time. There are a few ways to go about this.\n",
    "\n",
    "- '.describe()' will print out the summary statistics of your variable.\n",
    "- 'boxplots' can be used to visualize the summary statistics but not the most intuitive to understand the distribution. \n",
    "- 'histogram' are great to see the distribution, but understanding the summary statistics can be a challenge.\n",
    "\n",
    "Using multiple approaches can help get a better understanding of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a84f5-9371-4a58-9fa7-324210c03019",
   "metadata": {},
   "outputs": [],
   "source": [
    "CQ_attrib.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef87dee-5121-4176-b457-9cbed7e625e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CQ_attrib['AREA_SQKM'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f279f74-6186-4092-a96c-010eb6495daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 3 plots using the area attribute ('AREA_SQKM'). \n",
    "fig, axs = plt.subplots(1,3, figsize=(10, 3))\n",
    "\n",
    "ax1 = plt.subplot(1,3,1)\n",
    "sns.histplot(data=CQ_attrib, x=\"AREA_SQKM\", ax=ax1)\n",
    "ax1.set_ylim(0,20000)\n",
    "\n",
    "ax2 = plt.subplot(1,3,2)\n",
    "sns.histplot(x=np.log10(CQ_attrib['AREA_SQKM']), ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Log AREA SQKM')\n",
    "\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "sns.boxplot(data=CQ_attrib, x='AREA_SQKM', ax=ax3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327c782-48a9-4f74-aa05-a4d6c02b7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create histograms and boxplots for 'UrbLU_pct', 'AgLU_pct, and 'NatLU_pct'\n",
    "\n",
    "fig, axs = plt.subplots(3,2, figsize=(6, 6))\n",
    "\n",
    "# Urban LU\n",
    "ax1 = plt.subplot(3,2,1)\n",
    "#sns.histplot(data=*, x=*, ax=ax1)\n",
    "ax2 = plt.subplot(3,2,2)\n",
    "#sns.boxplot(data=*, x=*, ax=ax2)\n",
    "\n",
    "\n",
    "# Agricultural LU\n",
    "ax3 = plt.subplot(3,2,3)\n",
    "#sns.histplot(data=*, x=*, ax=ax3)\n",
    "ax4 = plt.subplot(3,2,4)\n",
    "#sns.boxplot(data=*, x=*, ax=ax4)\n",
    "\n",
    "# Natural LU\n",
    "ax3 = plt.subplot(3,2,3)\n",
    "#sns.histplot(data=*, x=*, ax=ax5)\n",
    "ax4 = plt.subplot(3,2,4)\n",
    "#sns.boxplot(data=*, x=*, ax=ax6)\n",
    "\n",
    "# Set your axis limits using the following syntax. ax1 refers to the first plot.\n",
    "# ax1.set_ylim(0,100) \n",
    "# ax2.set_xlim(0,75) \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33101fb-1a87-4b61-9945-301cf4998c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at Urban land use and wastewater treatment plant density. \n",
    "fig, axs = plt.subplots(2,2, figsize=(6, 6))\n",
    "\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "sns.histplot(data=CQ_attrib, x=\"UrbLU_pct\", ax=ax1)\n",
    "\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "sns.boxplot(data=CQ_attrib, x=\"UrbLU_pct\", ax=ax2)\n",
    "\n",
    "# WWTPd_100km2\n",
    "#ax3 = plt.subplot(2,2,3)\n",
    "#sns.histplot(data=CQ_attrib, x=#, ax=ax3)\n",
    "\n",
    "#ax4 = plt.subplot(2,2,4)\n",
    "#sns.boxplot(data=CQ_attrib, x=#,, ax=ax4)\n",
    "\n",
    "# Set your axis limits using the following syntax. ax1 refers to the first plot.\n",
    "# ax1.set_xlim(0,100)\n",
    "# ax2.set_xlim(0,75) \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bae39b-1fc2-43b1-9e60-c7bfa5935ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving onto our fertilizer and manure nitrogen inputs, and tile drainage.\n",
    "# FERT_kgkm2, MANU_kgkm2, TD_pct\n",
    "fig, axs = plt.subplots(3,2, figsize=(6, 6))\n",
    "\n",
    "ax1 = plt.subplot(3,2,1)\n",
    "sns.histplot(data=CQ_attrib, x=\"FERT_kgkm2\", ax=ax1)\n",
    "ax1.set_ylim(0,10000)\n",
    "\n",
    "ax2 = plt.subplot(3,2,2)\n",
    "sns.boxplot(data=CQ_attrib, x=\"FERT_kgkm2\", ax=ax2)\n",
    "ax2.set_xlim(0,5000)\n",
    "\n",
    "ax3 = plt.subplot(3,2,3)\n",
    "#sns.histplot\n",
    "\n",
    "ax4 = plt.subplot(3,2,4)\n",
    "#sns.boxplot\n",
    "\n",
    "ax5 = plt.subplot(3,2,5)\n",
    "#sns.histplot\n",
    "\n",
    "ax6 = plt.subplot(3,2,6)\n",
    "#sns.boxplot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16d1fa-aabc-4552-a1a7-08908a580b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we will look at the static attributes\n",
    "# BFI_AVE, SLOPE_pct, AI, medianGWD_m\n",
    "fig, axs = plt.subplots(4,2, figsize=(6, 8))\n",
    "\n",
    "ax1 = plt.subplot(4,2,1)\n",
    "#sns.histplot\n",
    "\n",
    "ax2 = plt.subplot(4,2,2)\n",
    "#sns.boxplot\n",
    "\n",
    "ax3 = plt.subplot(4,2,3)\n",
    "#sns.histplot\n",
    "\n",
    "ax4 = plt.subplot(4,2,4)\n",
    "#sns.boxplot\n",
    "\n",
    "ax5 = plt.subplot(4,2,5)\n",
    "#sns.histplot\n",
    "\n",
    "ax6 = plt.subplot(4,2,6)\n",
    "#sns.boxplot\n",
    "\n",
    "ax7 = plt.subplot(4,2,7)\n",
    "#sns.histplot\n",
    "\n",
    "ax8 = plt.subplot(4,2,8)\n",
    "#sns.boxplot\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea8ff6-c0b3-409f-a28b-1e130864f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to look at multivariate relationships, pairwise relationships we can make a pairplot of the attributes.\n",
    "CQ_attrib_filtered = CQ_attrib[['LAT', 'LONG', 'UrbLU_pct', 'NatLU_pct', 'AgLU_pct', 'FERT_kgkm2',\n",
    "       'MANU_kgkm2', 'BFI_AVE', 'SLOPE_pct', 'AI', 'medianGWD_m', 'TD_pct', 'WWTPd_100km2']]\n",
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(CQ_attrib_filtered, corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026dd8b-fa85-4cc6-99c3-d816edabc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = CQ_attrib_filtered.corr().stack().reset_index(name=\"correlation\")\n",
    "\n",
    "# Draw each cell as a scatter point with varying size and color\n",
    "g = sns.relplot(\n",
    "    data=corr_mat,\n",
    "    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n",
    "    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n",
    ")\n",
    "\n",
    "# Tweak the figure to finalize\n",
    "g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "g.despine(left=True, bottom=True)\n",
    "g.ax.margins(.02)\n",
    "for label in g.ax.get_xticklabels():\n",
    "    label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ad0e2-a748-4dc8-a0bf-7413e9989306",
   "metadata": {},
   "source": [
    "#### What do we see?\n",
    "- Latitude and Longitude are...\n",
    "- Land use and latitude/longitude are...\n",
    "- Percent slope of the watershed is correlated with agricultural LU and inputs. Why?\n",
    "- Agricultural land use and tile drainage are ...\n",
    "- Aridity index, groundwater depth, and tile drainage are correlated.\n",
    "- At this scale, urban land use (LU) and WWTP density are...\n",
    "- Fertilizer and manure are correlated with WWTP density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabca983-4b44-408e-8337-8c645b4d4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the flows and concentrations!\n",
    "plt.rcdefaults() # Setting plotting properties back to default\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(6, 3))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.histplot(x=np.log10(CQ_attrib['Q_m3s']), ax=ax1)\n",
    "plt.xlabel('Log Flow rate')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.boxplot(data=CQ_attrib, x=\"Q_m3s\", ax=ax2)\n",
    "ax2.set_xlim(0,100)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed239d81-2681-4a28-9cc7-fc92d39b1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CQ_attrib['Q_m3s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c276bc3-498c-411f-8251-89cd64f2e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(6, 3))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.histplot(x=np.log10(CQ_attrib['Conc']), ax=ax1)\n",
    "plt.xlabel('Log Conc (mg-N/L)')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.boxplot(data=CQ_attrib, x=\"Conc\", ax=ax2)\n",
    "plt.xlabel('Conc (mg-N/L)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69285e-855a-476b-8693-abe11f23518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CQ_attrib['Conc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ca378-4e0e-4542-b11d-38c2984b3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the highest concentrations?\n",
    "\n",
    "# Let's groupby 'SITE' to aggregate each site's concentation records.\n",
    "meanC = CQ_attrib.groupby('SITE')['Conc'].mean().reset_index()\n",
    "meanC_shp = wtshd_f.merge(meanC, on='SITE')\n",
    "\n",
    "# Creating a map to visualize our results.\n",
    "c_lim = q3 = CQ_attrib['Conc'].describe()['75%']\n",
    "base = US_boundary.plot(color='grey', edgecolor='black')\n",
    "meanC_shp.plot(ax=base, \n",
    "               column='Conc', \n",
    "               cmap='OrRd',\n",
    "               legend = True,\n",
    "               vmin=0,\n",
    "               vmax=c_lim,\n",
    "               edgecolor='none',\n",
    "               legend_kwds={\"label\": \"Mean Concentration (mg-N/L)\", \n",
    "                            \"orientation\": \"horizontal\"})\n",
    "\n",
    "# Contiguous US\n",
    "base.set_xlim(-3*10**6, 3*10**6)\n",
    "base.set_ylim(0, 3.5*10**6)\n",
    "\n",
    "# You miss some nuance looking at the map at this scale. Change the x and y-lims to inspect a few areas. \n",
    "# Midwest\n",
    "#base.set_xlim(-0.5*10**6, 1.25*10**6)\n",
    "#base.set_ylim(1.5*10**6, 3*10**6)\n",
    "\n",
    "# Western US\n",
    "#base.set_xlim(-3*10**6,0.5*10**6)\n",
    "#base.set_ylim(0.5*10**6, 2.25*10**6)\n",
    "\n",
    "# Texas\n",
    "#base.set_xlim(-1*10**6,0.25*10**6)\n",
    "#base.set_ylim(0, 1.5*10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79055a-6c26-4ca2-9f70-e84968f68597",
   "metadata": {},
   "source": [
    "#### What do we see?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369b6ce-652b-43fb-bee6-81f75174625e",
   "metadata": {},
   "source": [
    "### Developing research questions\n",
    "Working with open data is faster than conducting your own data collection, but it comes with uncertainties about data availability and the insights the data can provide. The resolution, both spatial and temporal, can limit the questions you can answer. The data itself can also impose limitations on your research scope. In many of my research projects, I don't follow the traditional hypothesis-testing approach. To avoid spending excessive time trying to make my data fit my needs, I let the data guide the questions I _can_ answer.\n",
    "\n",
    "**Summary of data:** \n",
    "- ...\n",
    "  \n",
    "**Questions we can ask:**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da3771-3c7f-420f-8f0b-cf1c13748a9b",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Power Law Slope (β)\n",
    "Feature engineering refers to the process of transforming raw data into new features (attributes) to gain better insights and enhance model performance. This term is widely used in the machine learning space, as it is a crucial step in improving model accuracy and effectiveness.\n",
    "\n",
    "One of the features we will generate is the C-Q Slope (β):\n",
    "\n",
    "β is the exponent in the power-law equation (C = aQ^β), where C represents concentration and Q represents flow. We can determine this slope by regressing C and Q in the log-log space.\n",
    "\n",
    "\n",
    "![CQ-relationship by Speirs et al.](../INPUTS/schematics/CQ_relationship.png)\n",
    "\n",
    "(figure recreated from Speirs et al.)\n",
    "\n",
    "To help you understand the theoretical 'β' metric in practical terms, here are explanations of the different regimes. These are generalized and can be more nuanced in reality.\n",
    "\n",
    "**Enrichment regime**: As flow increases, so does concentration. In this regime, excess nitrogen in the landscape is transport-limited. Thus, during a storm, the excess nitrogen is flushed and transported to the river.\n",
    "\n",
    "**Constant regime**: As flow increases, concentration remains constant. In this regime, as runoff increases, there is sufficient nitrogen available for transport, maintaining a stable concentration. This regime often exhibits the highest concentrations due to an excess nitrogen store.\n",
    "\n",
    "**Dilution regime**: As flow increases, concentration decreases. In this regime, there is usually a fixed mass of nitrogen, and as flows increase, this mass is diluted. For example, if a wastewater treatment plant has a constant mass loading, increased flow will dilute this load, reducing concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af441922-0ae0-4fe6-bc02-ccd936c869a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this looks with real data. You can choose two of your own or use '04176500' and '02096960'.\n",
    "# Recall the column names: 'Q_m3s', 'Conc', 'YEAR'\n",
    "fig, axs = plt.subplots(2,2, figsize=(6, 6))\n",
    "\n",
    "# Isolating the station data.\n",
    "CQ_Site = CQ_attrib.loc[CQ_attrib['SITE'] == '04176500']\n",
    "\n",
    "# Use a scatter plot to visualize the results from  'CQ_Site'. You can use c='YEAR' to color each marker based on the year. \n",
    "\n",
    "# plt1 = axs[0, 0].scatter(data=*, x=*, y=*, c=*,  alpha=0.5)\n",
    "# plt.colorbar(plt1, ax=axs[0, 0], orientation ='horizontal')\n",
    "# Add a title using this syntax: axs[0, 0].set_title('**')\n",
    "\n",
    "# You can also plot the log-log space by transforming Q and C (x=np.log10(CQ_Site['Q_m3s']) and y=np.log10(CQ_Site['Conc']))\n",
    "#axs[0, 1].scatter(data=*, x=*, y=*, c=*,  alpha=0.5)\n",
    "\n",
    "CQ_Site = CQ_attrib.loc[CQ_attrib['SITE'] == '02096960']\n",
    "\n",
    "# Plotting another station\n",
    "plt2 = axs[1, 0].scatter(data=CQ_Site, x='Q_m3s', y='Conc', c='YEAR',  alpha=0.5)\n",
    "plt.colorbar(plt2, ax=axs[1, 0], orientation ='horizontal')\n",
    "\n",
    "# Plotting in log-log space\n",
    "axs[1, 1].scatter(x=np.log10(CQ_Site['Q_m3s']), y=np.log10(CQ_Site['Conc']), c=CQ_Site['YEAR'],alpha=0.5)\n",
    "\n",
    "axs[1, 1].set_title('Neg. Power law b metric')\n",
    "axs[0, 1].set_ylabel('log C')\n",
    "axs[1, 1].set_ylabel('log C')\n",
    "axs[1, 1].set_xlabel('log Q')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae3691-6453-4b95-b708-cde155d08d37",
   "metadata": {},
   "source": [
    "How do the watersheds differ? What are the signs of their β slopes?\n",
    "\n",
    "Let's go ahead and derive β for all sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3812f-a275-4b1c-9d34-4d183931e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_PowerLaw(group):\n",
    "    X = np.log(group['Q_m3s']).values.reshape(-1, 1)\n",
    "    y = np.log(group['Conc']).values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    group['b'] = model.coef_[0]\n",
    "    return group\n",
    "\n",
    "# Apply the log-transformed linear regression function to each group. Add as a columns.\n",
    "CQ_attrib = CQ_attrib.groupby('SITE').apply(fit_PowerLaw, include_groups=False).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8014acf-84ba-4bd1-a90b-0c3f6e3dc8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the attributes so each site only has 1 record.\n",
    "CQ_attrib_mean = CQ_attrib.groupby('SITE').agg({'AREA_SQKM':'first', \n",
    "                                                'HUC02':'first', \n",
    "                                                'Q_m3s':'mean',\n",
    "                                                'Conc': 'mean', \n",
    "                                                 'b': 'first',\n",
    "                                                 'UrbLU_pct':'mean',\n",
    "                                                 'NatLU_pct':'mean',\n",
    "                                                 'AgLU_pct':'mean',\n",
    "                                                 'FERT_kgkm2':'mean',\n",
    "                                                 'MANU_kgkm2':'mean',\n",
    "                                                 'BFI_AVE':'first',\n",
    "                                                 'SLOPE_pct':'first',\n",
    "                                                 'AI':'first',\n",
    "                                                 'medianGWD_m':'first',\n",
    "                                                 'TD_pct':'first',\n",
    "                                                 'WWTPd_100km2':'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e48c61-9189-4345-b8c8-fb0f026c3f5f",
   "metadata": {},
   "source": [
    "### Spatial pattern CQ relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7f76e-dac9-48a6-b023-3a179d0a59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by looking at the spatial distribution of the 'b' metric\n",
    "b_shp = wtshd_f.merge(CQ_attrib_mean, on='SITE')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "base = US_boundary.plot(color='white', edgecolor='black')\n",
    "b_shp.plot(ax=base,\n",
    "           column='b', \n",
    "           cmap='RdYlBu',\n",
    "           legend =  True,\n",
    "           vmin=-1,\n",
    "           vmax=1,\n",
    "           edgecolor='none',\n",
    "           legend_kwds={\"label\": \"Power law 'b' metric\", \n",
    "                        \"orientation\": \"horizontal\"})\n",
    "\n",
    "base.set_xlim(-3*10**6, 3*10**6)\n",
    "base.set_ylim(0, 3.5*10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d4c94-255e-40f0-a3e8-df0bd097fc9c",
   "metadata": {},
   "source": [
    "We can see that **enrichment** slope (positive b) is more common in the midwest. Outside of this region we see a lot more **dilution export**.\n",
    "\n",
    "Doing another round of 'b metric' vs. watershed properties could help us understand the drivers of these export patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbd2fe-5fb2-4a96-bb33-1f42174913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which attributes would you like to plot?\n",
    "CQ_attrib.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935d579-7e79-4af2-bcc5-a72aab83649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this looks with real data\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Toggle the attributes\n",
    "attribute = 'UrbLU_pct'\n",
    "\n",
    "CQ_attrib_mean = CQ_attrib_mean.sort_values(by=[attribute])\n",
    "plt.scatter(data=CQ_attrib_mean, x='Conc', y='b',c = attribute, alpha=0.5)\n",
    "plt.colorbar(orientation ='horizontal')\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('b')\n",
    "plt.xlabel('Attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149133d-dd7c-4934-a7e8-8c973fb531fb",
   "metadata": {},
   "source": [
    "Looking at β and different attributes, what can we say?\n",
    "- Larger watershed...\n",
    "- Natural land use...\n",
    "- Agricultural land use...\n",
    "- The β in agricultural land use plot and tile drainage plot look different? Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef8418-bb66-4d94-b164-fa4d69cc085c",
   "metadata": {},
   "source": [
    "#### Dimension Reduction to Understand Watershed Similarities\n",
    "Understanding export behaviors with 12 different drivers and their interactions over 50 years can be challenging. To simplify this complexity, we can use dimension reduction techniques, specifically unsupervised machine learning.\n",
    "\n",
    "We will apply k-means clustering to group events with similar export patterns.\n",
    "\n",
    "The first step is to normalize the data, which involves selecting an appropriate method. Recall that many features have high extremes, which can pose issues for the k-means distance measure. To address this, we will set an upper limit on the values and then linearly transform them to fall between 0 and 1 for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729e67a-2994-4ad1-b4a1-519eac187e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the data. First we will cap some of the data, then we will use linear scaling to put all the data between [0,1]\n",
    "def linearNormalization(data, feature, min_val=None, max_val=None):\n",
    "    if min_val is not None:\n",
    "        data[feature] = np.maximum(data[feature], min_val)\n",
    "    if max_val is not None:\n",
    "        data[feature] = np.minimum(data[feature], max_val)\n",
    "        \n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    data[feature] = (data[feature] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93c12e-d3cb-4642-82e7-393107853d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the attributes we want to use in our clustering algorithm \n",
    "CQ_attrib['logAREA_SQKM'] = np.log10(CQ_attrib['AREA_SQKM'])\n",
    "attributes =  ['Conc', 'logAREA_SQKM', 'UrbLU_pct', 'NatLU_pct', 'AgLU_pct', 'TD_pct', 'FERT_kgkm2',\n",
    "               'MANU_kgkm2', 'BFI_AVE', 'SLOPE_pct', 'AI', 'medianGWD_m', 'WWTPd_100km2', 'b']\n",
    "kMeans_data = CQ_attrib[attributes].copy()\n",
    "\n",
    "# Setting upper limits to some of the data so we can apply a linear normalization without havies issues with large outliers.\n",
    "max_values = {\n",
    "    'Conc': 10,\n",
    "    'FERT_kgkm2': 5000,\n",
    "    'MANU_kgkm2': 5000,\n",
    "    'SLOPE_pct':100, \n",
    "    'AI':2, \n",
    "    'medianGWD_m':30, \n",
    "    'WWTPd_100km2':1,\n",
    "    'b':2}\n",
    "min_values = {'b':-2}\n",
    "\n",
    "# Normalizing the data\n",
    "for feature in attributes:\n",
    "    minVal = min_values.get(feature)\n",
    "    maxVal = max_values.get(feature)\n",
    "    linearNormalization(kMeans_data, feature, minVal, maxVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb154733-9dbb-44ec-b421-b421a72cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data to make sure it looks ok!\n",
    "sns.pairplot(kMeans_data, corner=True)\n",
    "sns.set(font_scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce796e-931d-463b-ab34-88c8fc83f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset defaults\n",
    "plt.rcdefaults()\n",
    "\n",
    "# We need to tune the k hyperparameter to find the optimal number of clusters. \n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(kMeans_data)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the hyperparameter tuning error results\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(range(1, 11, 1), sse, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distances (Inertia)')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca009c0-5073-4864-a89b-63e74272fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing 'k' is a subjective choice. But 8 clusters looks like a good start. \n",
    "optimal_k = 8\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(kMeans_data)\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "CQ_attrib['Cluster'] = kmeans.labels_\n",
    "def cust_mode(series):\n",
    "    return series.mode().iloc[0] if not series.mode().empty else np.nan\n",
    "\n",
    "# Let's groupby the method based on site. We will take the MODE of the cluster for each site because it's a categorical data. \n",
    "CQ_attrib_modeClust = CQ_attrib.groupby('SITE').agg({'logAREA_SQKM':'first', \n",
    "                                            'HUC02':'first', \n",
    "                                            'Q_m3s':'mean',\n",
    "                                            'Conc': 'mean', \n",
    "                                            'b': 'first',\n",
    "                                            'UrbLU_pct':'mean',\n",
    "                                            'NatLU_pct':'mean',\n",
    "                                            'AgLU_pct':'mean',\n",
    "                                            'FERT_kgkm2':'mean',\n",
    "                                            'MANU_kgkm2':'mean',\n",
    "                                            'BFI_AVE':'first',\n",
    "                                            'SLOPE_pct':'first',\n",
    "                                            'AI':'first',\n",
    "                                            'medianGWD_m':'first',\n",
    "                                            'TD_pct':'first',\n",
    "                                            'WWTPd_100km2':'first',\n",
    "                                            'Cluster':cust_mode}\n",
    "                                            ).reset_index()\n",
    "\n",
    "CQ_attrib_modeClust['Cluster'] = CQ_attrib_modeClust['Cluster'].astype('category')\n",
    "CQ_attrib_modeClust.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbf555-a260-4839-bc87-5efa056fc4e4",
   "metadata": {},
   "source": [
    "#### Using the clusters to generalize the different behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07514e0a-7f54-42f6-b385-d128a15c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to plot the bar plots of each clusters for each variable. \n",
    "CQ_attrib_long = pd.melt(CQ_attrib_modeClust, id_vars='Cluster', \n",
    "                         value_vars=['Conc', 'logAREA_SQKM', 'b', 'UrbLU_pct', \n",
    "                                     'NatLU_pct', 'AgLU_pct', 'FERT_kgkm2', 'MANU_kgkm2', \n",
    "                                     'BFI_AVE', 'SLOPE_pct', 'AI', 'medianGWD_m', \n",
    "                                     'TD_pct', 'WWTPd_100km2'],\n",
    "                         var_name='Attribute', value_name='Value')\n",
    "\n",
    "attributes = ['logAREA_SQKM', 'Conc', 'UrbLU_pct', \n",
    "              'NatLU_pct', 'AgLU_pct', 'TD_pct', 'FERT_kgkm2', 'MANU_kgkm2', \n",
    "              'BFI_AVE', 'SLOPE_pct', 'AI', 'medianGWD_m', 'WWTPd_100km2','b']\n",
    "\n",
    "# Number of rows and columns for subplots\n",
    "n_rows = 4\n",
    "n_cols = 7\n",
    "\n",
    "# Manually setting color palette to match the map's cmap. \n",
    "palette = [\n",
    "    (0.27, 0.004, 0.33),\n",
    "    (0.27, 0.20, 0.49),\n",
    "    (0.21, 0.36, 0.55),\n",
    "    (0.15, 0.50, 0.56),\n",
    "    (0.12, 0.63, 0.53),\n",
    "    (0.29, 0.76, 0.43),\n",
    "    (0.63, 0.85, 0.22),\n",
    "    (0.99, 0.91, 0.15)\n",
    "]\n",
    "\n",
    "# Using GridSpec to organize the figure to have the boxplots at the top and map at the bottom. \n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = gridspec.GridSpec(n_rows, n_cols, figure=fig, height_ratios=[1,1, 1, 1])\n",
    "\n",
    "# Plot each attribute in a separate subplot\n",
    "for i, attribute in enumerate(attributes):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    sns.boxplot(x='Cluster', \n",
    "                y='Value', \n",
    "                data=CQ_attrib_long[CQ_attrib_long['Attribute'] == attribute], \n",
    "                ax=ax, \n",
    "                showfliers=False, \n",
    "                palette=palette,\n",
    "                hue='Cluster',\n",
    "                dodge=False,\n",
    "               legend=False)\n",
    "    ax.set_title(attribute)\n",
    "    ax.set_xlabel('Cluster')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Merge datasets for spatial plot\n",
    "b_shp = wtshd_f.merge(CQ_attrib_modeClust, on='SITE')\n",
    "\n",
    "# Create the subplot for the map, occupying the entire last row\n",
    "ax_map = fig.add_subplot(gs[2:, :])\n",
    "base = US_boundary.plot(ax=ax_map, color='white', edgecolor='black')\n",
    "b_shp.plot(ax=base,\n",
    "           column='Cluster', \n",
    "           cmap='viridis',\n",
    "           legend=True)\n",
    "ax_map.set_xlim(-3*10**6, 3*10**6)\n",
    "ax_map.set_ylim(0, 3.5*10**6)\n",
    "ax_map.set_title('Spatial Distribution of the clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da963a1-aafc-4208-8679-50a01a1f0a1c",
   "metadata": {},
   "source": [
    "## What do we see in our cluster analysis?\n",
    "We are starting to see some patterns emerge once we are able to apply dimension reduction. \n",
    "\n",
    "**Clusters 0**: Agricultural watersheds that are heavily tile-drained. These watersheds have high concentrations and highest fertilizer and manure inputs. These watersheds have a lot of nitrogen and thus have a lot extra to export. \n",
    "\n",
    "**Cluster 1**: Arid watersheds with deep groundwater and much of the watershed is natural land use. These watersheds have low concentration and high base flow index. \n",
    "\n",
    "**Cluster 2**: Highly urban watersheds but with low wasterwater treatment plants density. This explains why we might not see the dilution pattern expected with wastewater treatment plants. \n",
    "\n",
    "**Cluster 3**: Agricultural watersheds that are not heavily tile-drained. These watersheds have a more constant export, in comparison to Cluster 0.\n",
    "\n",
    "**Cluster 4**: Agricultural watersheds that are moderately tile-drained. Spatially they surround Cluster 0 watersheds, so they are similar but are less influenced by the effects of tile drainage on export. \n",
    "\n",
    "**Cluster 5**: Urban watersheds with more higher wastewater treatment density. Median b metric is negative, which means these watersheds tend to have more of a dilution patterns. These watersheds tend to be smaller, which is perhaps why we can see the effects of urban nitrogen sources on the concentration.\n",
    "\n",
    "**Cluster 6**: Similar to cluster 1 but in temperate climates (low aridity index). \n",
    "\n",
    "**Cluster 7**: Arid agricultural watersheds. These watersheds have similar properties to Cluster 0 and Cluster 3, but because they are arid the groundwater residence time is much longer, thus we are seeing very different signals compared to more temperate agricultural watersheds. \n",
    "\n",
    "### Final Thoughts on Spatial Patterns\n",
    "Using a series of approaches, we were able to abstract and find some patterns in our dataset. The b metric helped us a _little_, but there isn't much variation across the clusters. That's okay! Explorers don't always strike gold right away! \n",
    "\n",
    "What are the next steps, you might ask? If this were my project, I would return to the feature engineering step and see what other type of features I can generate to explore the effects of watershed properties on C-Q relationships. Additionally, after seeing some interesting patterns from our analysis, I might try to find other watershed attributes to dig deeper into some of the threads I've created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186a657-0007-4bfc-801a-daa3e8b0793b",
   "metadata": {},
   "source": [
    "### Temporal pattern CQ relationships\n",
    "We will not be exploring CQ relationships over time. But for those who are interested in digging further or who are experienced coders, I encourage you to explore the temporal patterns of CQ relationships. Here are some prompt to help get your started:\n",
    "- How has 'β' metric change over time? Split the data by decade and track the changes in the watershed.\n",
    "- Do we see the influence of changing land use or inputs (Fertilizer or manure) on this relationship? \n",
    "- Do we see annual variability in the 'β' metric in temporate or arid catchments? Do we see more annual variability in hydrologically connected landscapes? Does the slow movement of arid system act as a modulator of changing relationships?\n",
    "- How does CQ relationship change seasonally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507b5e5-bab7-4546-aad5-92aa5e613ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_DataExploration",
   "language": "python",
   "name": "workshop_dataexploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
